# commented args represent values filled out
# by train task at run time. To build a functional
# standalone config, add these in

model:
  arch:
    class_path: train.architectures.Autoencoder
    init_args:
      hidden_channels: [8, 16, 32, 64]
  loss:
    class_path: train.metrics.PsdRatio
    init_args:
      fftlength: 2
      asd: true
  metric:
    class_path: train.metrics.OnlinePsdRatio
    init_args:
      filter_pad: 0.5
      edge_pad: 0.25
  # if we specified an early-stopping patience interval, add early stopping
  patience: 20
data:
  # loading args
  # fname:
  # H1
  channels: [
    H1:GDS-CALIB_STRAIN,
    H1:PEM-CS_MAINSMON_EBAY_1_DQ,
    H1:ASC-CHARD_P_OUT_DQ,
    H1:ASC-CHARD_Y_OUT_DQ,
    H1:ASC-CSOFT_P_OUT_DQ,
    H1:ASC-CSOFT_Y_OUT_DQ,
    H1:ASC-DHARD_P_OUT_DQ,
    H1:ASC-DHARD_Y_OUT_DQ,
    H1:ASC-DSOFT_P_OUT_DQ,
    H1:ASC-DSOFT_Y_OUT_DQ,
    H1:ASC-INP1_P_OUT_DQ,
    H1:ASC-INP1_Y_OUT_DQ,
    H1:ASC-MICH_P_OUT_DQ,
    H1:ASC-MICH_Y_OUT_DQ,
    H1:ASC-PRC1_P_OUT_DQ,
    H1:ASC-PRC1_Y_OUT_DQ,
    H1:ASC-PRC2_P_OUT_DQ,
    H1:ASC-PRC2_Y_OUT_DQ,
    H1:ASC-SRC1_P_OUT_DQ,
    H1:ASC-SRC1_Y_OUT_DQ,
    H1:ASC-SRC2_P_OUT_DQ,
    H1:ASC-SRC2_Y_OUT_DQ
    ]

  # L1
  channels: [
    L1:GDS-CALIB_STRAIN,
    L1:PEM-CS_MAINSMON_EBAY_1_DQ,
    L1:ASC-CHARD_P_OUT_DQ,
    L1:ASC-CHARD_Y_OUT_DQ,
    L1:ASC-CSOFT_P_OUT_DQ,
    L1:ASC-DHARD_P_OUT_DQ,
    L1:ASC-DHARD_Y_OUT_DQ,
    L1:ASC-DSOFT_P_OUT_DQ,
    L1:ASC-INP1_P_OUT_DQ,
    L1:ASC-MICH_P_OUT_DQ,
    L1:ASC-MICH_Y_OUT_DQ,
    L1:ASC-PRC1_P_OUT_DQ,
    L1:ASC-PRC1_Y_OUT_DQ,
    L1:ASC-PRC2_P_OUT_DQ,
    L1:ASC-PRC2_Y_OUT_DQ,
    L1:ASC-SRC1_P_OUT_DQ,
    L1:ASC-SRC1_Y_OUT_DQ,
    L1:ASC-SRC2_P_OUT_DQ,
    L1:ASC-SRC2_Y_OUT_DQ
    ]

  # K1
  channels: [
    K1:CAL-CS_PROC_DARM_STRAIN_DBL_DQ,
    K1:PEM-MIC_OMC_BOOTH_OMC_Z_OUT_DQ,
    K1:PEM-MIC_OMC_TABLE_AS_Z_OUT_DQ,
    K1:PEM-MIC_BS_BOOTH_BS_Z_OUT_DQ,
    K1:PEM-MIC_BS_FIELD_BS_Z_OUT_DQ,
    K1:PEM-MIC_BS_TABLE_POP_Z_OUT_DQ,
    K1:PEM-MIC_IXC_BOOTH_IXC_Z_OUT_DQ,
    K1:PEM-MIC_IXC_FIELD_IXC_Z_OUT_DQ,
    K1:PEM-MIC_IYC_BOOTH_IYC_Z_OUT_DQ,
    K1:PEM-MIC_TMSX_TABLE_TMS_Z_OUT_DQ,
    K1:PEM-VOLT_AS_TABLE_GND_OUT_DQ,
    K1:PEM-VOLT_IMCREFL_TABLE_GND_OUT_DQ,
    K1:PEM-VOLT_ISS_TABLE_GND_OUT_DQ,
    K1:PEM-VOLT_OMC_CHAMBER_GND_OUT_DQ,
    K1:PEM-VOLT_PSL_TABLE_GND_OUT_DQ,
    K1:PEM-VOLT_REFL_TABLE_GND_OUT_DQ
    ]
  train_duration: 4096
  test_duration: 8192
  train_stride: 0.25 vs 0.0625
  valid_frac: 0.1 vs 0.33 vs 0.25
  # The rate at which to sample kernels from the witness
  # and strain timeseries, in Hz. Must be less than or
  # equal to `sample_rate`.
  inference_sampling_rate: 64
  start_offset: 0

  # preprocessing args
  batch_size: 32 vs 512
  kernel_length: 8
  filt_order: 8
  freq_low: [55]
  freq_high: [65]
optimizer:
  lr: 1e-3 vs 0.512
lr_scheduler:
  pct_start: 0.33
trainer:
  # by default, use a local CSV logger.
  # Options in train task for using a
  # wandb logger instead
  logger:
    - class_path: lightning.pytorch.loggers.CSVLogger
      init_args:
        # save_dir:
        flush_logs_every_n_steps: 10
  # devices:
  # strategy: set to ddp if len(devices) > 1
  # don't get much of a boost from MP training right
  # now because our convolutions aren't really large enough
  # precision: 16-mixed
  accelerator: auto
  # Maximum number of epochs over which to train.
  max_epochs: 20 vs 100 vs 40 (trainer)
  check_val_every_n_epoch: 1
  log_every_n_steps: 20
  benchmark: true
